{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d907acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d355c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Generator\n",
    "from operator import itemgetter\n",
    "from langchain_core.messages import trim_messages\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableWithMessageHistory, RunnablePassthrough\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c231ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865081f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a helpful assistant '{llm_name}' who responds to questions in not more than 20 sentences. You can use markdown and code blocks to format your answers. You can also use emojis to make your answers more engaging. Please be concise and clear in your responses.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{new_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set-up LLM:\n",
    "llm = ChatOllama(model=\"qwen2.5:14b\")\n",
    "\n",
    "# Output parser:\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1f327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ad03dfe",
   "metadata": {},
   "source": [
    "### Stateless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb72b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stateless Chain:\n",
    "(template | llm | parser).invoke(\n",
    "    {\n",
    "        \"llm_name\": \"Random\",\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(content=\"What is the capital of France?\"),\n",
    "            AIMessage(content=\"The capital of France is Paris.\"),\n",
    "            HumanMessage(content=\"Germany?\"),\n",
    "            AIMessage(content=\"The capital of Germany is Berlin.\"),\n",
    "        ],\n",
    "        \"new_input\": \"India?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d96ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4f75c3d",
   "metadata": {},
   "source": [
    "### Stateful:\n",
    "- If you open LangSmith, Don't worry about the System prompt being not present in the Trimmer's output.\n",
    "- The reason is that we pass Sys Prompt from ChatPromptTemplate\n",
    "- Whereas trimmer works on Chat History from get_session_history\n",
    "- So, sys prompt is injected later which can be checked in ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bc5c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "        (\"system\", \"You are a helpful assistant '{llm_name}' who responds to questions in not more than 20 sentences. You can use markdown and code blocks to format your answers. You can also use emojis to make your answers more engaging. Please be concise and clear in your responses.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{new_input}\")\n",
    "    ]\n",
    ")\n",
    "llm = ChatOllama(model=\"qwen2.5:14b\")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a881c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_chat_hist = ChatMessageHistory()\n",
    "\n",
    "def get_session_history() -> BaseChatMessageHistory:\n",
    "    global lc_chat_hist\n",
    "    if not \"lc_chat_hist\":\n",
    "        lc_chat_hist = ChatMessageHistory()\n",
    "    return lc_chat_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87610b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimmer:\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=100, strategy=\"last\",\n",
    "    token_counter=llm, include_system=True,\n",
    "    allow_partial=True, start_on=HumanMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec08902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stateful Chain:\n",
    "# chain = (\n",
    "#     RunnablePassthrough.assign(\n",
    "#         messages=itemgetter(\"chat_history\") | trimmer)\n",
    "#     | template\n",
    "#     | llm\n",
    "#     | parser\n",
    "# )\n",
    "\n",
    "chain = (\n",
    "    # Set messages key equal to chat_history\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"chat_history\") | trimmer)\n",
    "    # Set chat_history key equal to messages (default output key of trimmer)\n",
    "    | RunnablePassthrough.assign(chat_history=itemgetter(\"messages\"))\n",
    "    | template\n",
    "    | llm\n",
    "    | parser\n",
    ")\n",
    "\n",
    "llm_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"new_input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain (streaming):\n",
    "def get_resp(new_input: str, llm_name: str = \"ToddLLM\") -> Generator[str, None, None]:\n",
    "    global lc_chat_hist\n",
    "    resp = llm_with_history.stream({\n",
    "        \"new_input\": new_input,\n",
    "        \"llm_name\": llm_name\n",
    "    })\n",
    "    for ans in resp:\n",
    "        yield ans\n",
    "\n",
    "\n",
    "for ans in get_resp(\"Hello world! How are you?\", \"ToddLLM\"):\n",
    "    print(ans, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87092ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ans in get_resp(\"What was my name?\"):\n",
    "    print(ans, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff69d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ans in get_resp(\"Nope, I am Bhushan! What about you?\"):\n",
    "    print(ans, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ans in get_resp(\"Repeat my name?\"):\n",
    "    print(ans, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a559612",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ans in get_resp(\"Finally my chatbot is working with trimmer.\"):\n",
    "    print(ans, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62de4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_chat_hist.messages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
